birdcall_model_train.ipynb_
Files
..
Drop files to upload them to session storage
To run this colab, press the "Runtime" button in the menu tab and then press the "Run all" button.
Recognize Birdsong using Transfer Learning

Mounted at /content/gdrive

Setup Input Pipeline

set the base directory of the birdsong folders

Use ImageDataGenerator to rescale the images.

Create the train generator and specify where the train dataset directory, image size, batch size.

Create the validation generator with similar approach as the train generator with the flow_from_directory() method.

Found 1699 images belonging to 12 classes.
Found 419 images belonging to 12 classes.

((32, 32, 32, 3), (32, 12))

Save the labels in a file which will be downloaded later.

{'Arctic_Warbler': 0, 'Barn_Swallow': 1, 'Black-headed_Gull': 2, 'Black_Redstart': 3, 'Brambling': 4, 'Coal_Tit': 5, 'Common_Blackbird': 6, 'Common_Buzzard': 7, 'Common_Chaffinch': 8, 'Common_Grasshopper_Warbler': 9, 'Common_Whitethroat': 10, 'Common_Wood_Pigeon': 11}

Arctic_Warbler
Barn_Swallow
Black-headed_Gull
Black_Redstart
Brambling
Coal_Tit
Common_Blackbird
Common_Buzzard
Common_Chaffinch
Common_Grasshopper_Warbler
Common_Whitethroat
Common_Wood_Pigeon

Create the base model from the pre-trained convnets

Create the base model from the MobileNet V2 model developed at Google, and pre-trained on the ImageNet dataset, a large dataset of 1.4M images and 1000 classes of web images.

First, pick which intermediate layer of MobileNet V2 will be used for feature extraction. A common practice is to use the output of the very last layer before the flatten operation, the so-called "bottleneck layer". The reasoning here is that the following fully-connected layers will be too specialized to the task the network was trained on, and thus the features learned by these layers won't be very useful for a new task. The bottleneck features, however, retain much generality.

Let's instantiate an MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the include_top=False argument, we load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.
Feature extraction

You will freeze the convolutional base created from the previous step and use that as a feature extractor, add a classifier on top of it and train the top-level classifier.
Add a classification head
Compile the model

You must compile the model before training it. Since there are multiple classes, use a categorical cross-entropy loss.

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 30, 30, 32)        896       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 13, 13, 32)        9248      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 6, 6, 32)          0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 4, 4, 32)          9248      
_________________________________________________________________
flatten (Flatten)            (None, 512)               0         
_________________________________________________________________
dense (Dense)                (None, 128)               65664     
_________________________________________________________________
dense_1 (Dense)              (None, 12)                1548      
=================================================================
Total params: 86,604
Trainable params: 86,604
Non-trainable params: 0
_________________________________________________________________

Number of trainable variables = 10

Train the model

Epoch 1/40
54/54 [==============================] - 1408s 26s/step - loss: 2.2105 - accuracy: 0.2517 - val_loss: 2.0630 - val_accuracy: 0.3341
Epoch 2/40
54/54 [==============================] - 12s 230ms/step - loss: 1.3208 - accuracy: 0.5483 - val_loss: 1.8325 - val_accuracy: 0.4606
Epoch 3/40
54/54 [==============================] - 12s 228ms/step - loss: 0.9836 - accuracy: 0.6701 - val_loss: 1.9679 - val_accuracy: 0.4463
Epoch 4/40
54/54 [==============================] - 12s 231ms/step - loss: 0.8873 - accuracy: 0.7079 - val_loss: 2.0971 - val_accuracy: 0.4726
Epoch 5/40
54/54 [==============================] - 12s 226ms/step - loss: 0.7217 - accuracy: 0.7826 - val_loss: 2.0330 - val_accuracy: 0.4582
Epoch 6/40
54/54 [==============================] - 12s 226ms/step - loss: 0.6326 - accuracy: 0.7921 - val_loss: 2.2475 - val_accuracy: 0.4869
Epoch 7/40
54/54 [==============================] - 12s 230ms/step - loss: 0.5182 - accuracy: 0.8239 - val_loss: 2.1712 - val_accuracy: 0.5131
Epoch 8/40
54/54 [==============================] - 12s 226ms/step - loss: 0.4719 - accuracy: 0.8440 - val_loss: 2.4716 - val_accuracy: 0.4964
Epoch 9/40
54/54 [==============================] - 12s 226ms/step - loss: 0.3435 - accuracy: 0.8888 - val_loss: 2.6459 - val_accuracy: 0.5179
Epoch 10/40
54/54 [==============================] - 12s 226ms/step - loss: 0.3139 - accuracy: 0.9035 - val_loss: 2.4898 - val_accuracy: 0.5346
Epoch 11/40
54/54 [==============================] - 12s 226ms/step - loss: 0.3194 - accuracy: 0.9076 - val_loss: 2.4496 - val_accuracy: 0.4964
Epoch 12/40
54/54 [==============================] - 12s 230ms/step - loss: 0.2866 - accuracy: 0.9076 - val_loss: 2.9515 - val_accuracy: 0.5656
Epoch 13/40
54/54 [==============================] - 12s 225ms/step - loss: 0.2689 - accuracy: 0.9259 - val_loss: 3.0386 - val_accuracy: 0.5322
Epoch 14/40
54/54 [==============================] - 12s 227ms/step - loss: 0.1783 - accuracy: 0.9474 - val_loss: 3.2966 - val_accuracy: 0.5131
Epoch 15/40
54/54 [==============================] - 12s 227ms/step - loss: 0.1729 - accuracy: 0.9485 - val_loss: 3.3588 - val_accuracy: 0.4869
Epoch 16/40
54/54 [==============================] - 12s 231ms/step - loss: 0.1832 - accuracy: 0.9352 - val_loss: 3.3819 - val_accuracy: 0.5274
Epoch 17/40
54/54 [==============================] - 12s 226ms/step - loss: 0.1549 - accuracy: 0.9555 - val_loss: 3.3754 - val_accuracy: 0.5513
Epoch 18/40
54/54 [==============================] - 12s 227ms/step - loss: 0.2204 - accuracy: 0.9248 - val_loss: 3.8542 - val_accuracy: 0.5346
Epoch 19/40
54/54 [==============================] - 12s 227ms/step - loss: 0.1067 - accuracy: 0.9664 - val_loss: 3.9266 - val_accuracy: 0.5060
Epoch 20/40
54/54 [==============================] - 12s 230ms/step - loss: 0.1277 - accuracy: 0.9645 - val_loss: 3.9334 - val_accuracy: 0.5823
Epoch 21/40
54/54 [==============================] - 12s 228ms/step - loss: 0.0899 - accuracy: 0.9776 - val_loss: 4.1278 - val_accuracy: 0.5728
Epoch 22/40
54/54 [==============================] - 12s 226ms/step - loss: 0.0762 - accuracy: 0.9762 - val_loss: 3.9479 - val_accuracy: 0.5704
Epoch 23/40
54/54 [==============================] - 12s 226ms/step - loss: 0.1080 - accuracy: 0.9610 - val_loss: 4.1042 - val_accuracy: 0.5394
Epoch 24/40
54/54 [==============================] - 12s 226ms/step - loss: 0.0930 - accuracy: 0.9676 - val_loss: 4.4143 - val_accuracy: 0.5537
Epoch 25/40
54/54 [==============================] - 12s 225ms/step - loss: 0.0886 - accuracy: 0.9727 - val_loss: 4.3195 - val_accuracy: 0.5656
Epoch 26/40
54/54 [==============================] - 13s 238ms/step - loss: 0.0582 - accuracy: 0.9830 - val_loss: 4.7564 - val_accuracy: 0.5752
Epoch 27/40
54/54 [==============================] - 12s 229ms/step - loss: 0.0902 - accuracy: 0.9725 - val_loss: 4.8283 - val_accuracy: 0.5752
Epoch 28/40
54/54 [==============================] - 12s 228ms/step - loss: 0.0540 - accuracy: 0.9823 - val_loss: 4.8885 - val_accuracy: 0.5728
Epoch 29/40
54/54 [==============================] - 12s 230ms/step - loss: 0.0331 - accuracy: 0.9915 - val_loss: 4.8900 - val_accuracy: 0.5418
Epoch 30/40
54/54 [==============================] - 12s 229ms/step - loss: 0.0437 - accuracy: 0.9873 - val_loss: 5.0628 - val_accuracy: 0.5251
Epoch 31/40
54/54 [==============================] - 12s 232ms/step - loss: 0.0707 - accuracy: 0.9769 - val_loss: 4.8825 - val_accuracy: 0.5561
Epoch 32/40
54/54 [==============================] - 12s 228ms/step - loss: 0.0266 - accuracy: 0.9945 - val_loss: 5.0564 - val_accuracy: 0.5513
Epoch 33/40
54/54 [==============================] - 12s 227ms/step - loss: 0.0134 - accuracy: 0.9982 - val_loss: 5.3469 - val_accuracy: 0.5632
Epoch 34/40
54/54 [==============================] - 12s 227ms/step - loss: 0.0242 - accuracy: 0.9966 - val_loss: 4.8891 - val_accuracy: 0.5561
Epoch 35/40
54/54 [==============================] - 12s 228ms/step - loss: 0.1625 - accuracy: 0.9513 - val_loss: 5.4273 - val_accuracy: 0.5346
Epoch 36/40
54/54 [==============================] - 12s 229ms/step - loss: 0.0693 - accuracy: 0.9725 - val_loss: 4.8722 - val_accuracy: 0.5513
Epoch 37/40
54/54 [==============================] - 12s 227ms/step - loss: 0.0346 - accuracy: 0.9909 - val_loss: 5.5392 - val_accuracy: 0.5251
Epoch 38/40
54/54 [==============================] - 12s 227ms/step - loss: 0.0158 - accuracy: 0.9969 - val_loss: 5.5056 - val_accuracy: 0.5680
Epoch 39/40
54/54 [==============================] - 12s 227ms/step - loss: 0.0044 - accuracy: 0.9999 - val_loss: 5.8089 - val_accuracy: 0.5704
Epoch 40/40
54/54 [==============================] - 12s 227ms/step - loss: 0.0052 - accuracy: 0.9997 - val_loss: 6.0823 - val_accuracy: 0.5632

Learning curves

Let's take a look at the learning curves of the training and validation accuracy/loss when using the MobileNet V2 base model as a fixed feature extractor.
Fine tuning

In our feature extraction experiment, you were only training a few layers on top of an MobileNet V2 base model. The weights of the pre-trained network were not updated during training.

One way to increase performance even further is to train (or "fine-tune") the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset.
Un-freeze the top layers of the model

All you need to do is unfreeze the base_model and set the bottom layers be un-trainable. Then, recompile the model (necessary for these changes to take effect), and resume training.

Number of layers in the base model:  154

Compile the model

Compile the model using a much lower training rate.

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   
_________________________________________________________________
conv2d (Conv2D)              (None, 5, 5, 32)          368672    
_________________________________________________________________
dropout (Dropout)            (None, 5, 5, 32)          0         
_________________________________________________________________
global_average_pooling2d (Gl (None, 32)                0         
_________________________________________________________________
dense (Dense)                (None, 12)                396       
=================================================================
Total params: 2,627,052
Trainable params: 2,230,508
Non-trainable params: 396,544
_________________________________________________________________

Number of trainable variables = 58

Continue Train the model

Epoch 1/5
27/27 [==============================] - 271s 10s/step - loss: 4.1428 - accuracy: 0.1824 - val_loss: 2.6716 - val_accuracy: 0.3795
Epoch 2/5
27/27 [==============================] - 258s 10s/step - loss: 2.0142 - accuracy: 0.4305 - val_loss: 3.1628 - val_accuracy: 0.3317
Epoch 3/5
27/27 [==============================] - 258s 10s/step - loss: 1.3784 - accuracy: 0.5439 - val_loss: 3.3054 - val_accuracy: 0.3055
Epoch 4/5
27/27 [==============================] - 259s 10s/step - loss: 1.1913 - accuracy: 0.6027 - val_loss: 3.4033 - val_accuracy: 0.2912
Epoch 5/5
27/27 [==============================] - 257s 10s/step - loss: 1.0564 - accuracy: 0.6438 - val_loss: 3.4994 - val_accuracy: 0.2792

Convert to TFLite

Saved the model using tf.saved_model.save and then convert the saved model to a tf lite compatible format.

INFO:tensorflow:Assets written to: save/fine_tuning/assets

Download the converted model and labels

Let's take a look at the learning curves of the training and validation accuracy/loss, when fine tuning the last few layers of the MobileNet V2 base model and training the classifier on top of it. The validation loss is much higher than the training loss, so you may get some overfitting.

You may also get some overfitting as the new training set is relatively small and similar to the original MobileNet V2 datasets.
Summary:

    Using a pre-trained model for feature extraction: When working with a small dataset, it is common to take advantage of features learned by a model trained on a larger dataset in the same domain. This is done by instantiating the pre-trained model and adding a fully-connected classifier on top. The pre-trained model is "frozen" and only the weights of the classifier get updated during training. In this case, the convolutional base extracted all the features associated with each image and you just trained a classifier that determines the image class given that set of extracted features.

    Fine-tuning a pre-trained model: To further improve performance, one might want to repurpose the top-level layers of the pre-trained models to the new dataset via fine-tuning. In this case, you tuned your weights such that your model learned high-level features specific to the dataset. This technique is usually recommended when the training dataset is large and very similar to the orginial dataset that the pre-trained model was trained on.

activation, hint
